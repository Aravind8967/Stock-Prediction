===================================== Model by Grec hoc ===============================================

def model_train(self, x_train, y_train, x_valid, y_valid):
    model = Sequential([Input((3, 1)),
                        LSTM(64),
                        Dense(32,activation='relu'),
                        Dense(32, activation='relu'),
                        Dense(1)])
    
    model.compile(loss='mse',
                    optimizer=Adam(learning_rate=0.001),
                    metrics=['mean_absolute_error'])

    model.fit(x_train, y_train, validation_data=(x_valid, y_valid), epochs=100)
    return model


===================================== previous model (ChatGPT) ========================================

def model_train(self, x_train, y_train, x_valid, y_valid, window_size):
    """
    Build and train an LSTM-based model for stock price prediction.

    Parameters:
      x_train (np.array): Training data shaped as (num_samples, window_size, 1).
      y_train (np.array): Target values for training.
      x_valid (np.array): Validation data.
      y_valid (np.array): Target values for validation.
      window_size (int): The number of timesteps in each input sample.
    
    Returns:
      model: A trained Keras Sequential model.
    """

    # Build the Sequential model with dynamic input shape based on window_size.
    model = Sequential([
        # Input layer: expects sequences with 'window_size' timesteps and 1 feature per timestep.
        Input((window_size, 1)),
        # First LSTM layer with 64 units. Return sequences to feed into a second LSTM.
        LSTM(64, return_sequences=True),
        # Dropout layer to reduce overfitting by randomly setting 20% of inputs to 0.
        Dropout(0.2),
        # Second LSTM layer with 32 units; it does not return sequences as it outputs a fixed vector.
        LSTM(32),
        # Another Dropout layer for regularization.
        Dropout(0.2),
        # Dense layer with 32 neurons and ReLU activation for non-linear transformation.
        Dense(32, activation='relu'),
        # Final Dense layer with 1 neuron (linear activation by default) for continuous value prediction.
        Dense(1)
    ])

    # Optimizer: Adam with an initial learning rate.
    optimizer = Adam(learning_rate=0.001)

    # Compile the model using Mean Squared Error loss (common in regression) and track MAE.
    model.compile(loss='mse', optimizer=optimizer, metrics=['mean_absolute_error'])

    # Early stopping callback: stops training if the validation loss does not improve for 10 epochs.
    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
    
    # Learning rate scheduler: reduces the learning rate by a factor of 0.5 if no improvement in 5 epochs.
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)

    # Train the model. The 'validation_data' helps monitor performance on unseen data.
    model.fit(
        x_train, y_train,
        validation_data=(x_valid, y_valid),
        epochs=100,
        callbacks=[early_stop, reduce_lr]
    )
    
    return model



===================================== Letest model (Gemini) ==========================================

def model_train(self, x_train, y_train, x_valid, y_valid, window_size):
    model = Sequential([
        # Input layer: expects sequences with 'window_size' timesteps and 1 feature per timestep.
        Input((window_size, 1)),

        # Convolutional Layer: helps to extract local patterns.
        Conv1D(filters=64, kernel_size=3, activation='relu'),
        MaxPooling1D(pool_size=2),

        # Bidirectional LSTM Layer: captures patterns from both past and future.
        Bidirectional(LSTM(128, return_sequences=True)),
        BatchNormalization(),
        Dropout(0.3),

        # Second Bidirectional LSTM Layer.
        Bidirectional(LSTM(128, return_sequences=True)),
        BatchNormalization(),
        Dropout(0.3),

        # Third LSTM or fully connected layer, to reduce the dimensions.
        LSTM(64),
        BatchNormalization(),
        Dropout(0.3),

        # Dense layer with 64 neurons and ReLU activation.
        Dense(64, activation='relu'),
        BatchNormalization(),
        Dropout(0.2),

        # Final Dense layer with 1 neuron for continuous value prediction.
        Dense(1)
    ])
    # Optimizer: Adam with an initial learning rate.
    optimizer = Adam(learning_rate=0.0005)

    # Compile the model using Mean Squared Error loss (common in regression) and track MAE.
    model.compile(loss='mse', optimizer=optimizer, metrics=['mean_absolute_error'])

    # Early stopping callback: stops training if the validation loss does not improve for 10 epochs.
    early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)
    
    # Learning rate scheduler: reduces the learning rate by a factor of 0.5 if no improvement in 5 epochs.
    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=10, min_lr=1e-7)

    # Train the model. The 'validation_data' helps monitor performance on unseen data.
    model.fit(
        x_train, y_train,
        validation_data=(x_valid, y_valid),
        epochs=100,
        batch_size=32,
        callbacks=[early_stop, reduce_lr]
    )
    
    return model
